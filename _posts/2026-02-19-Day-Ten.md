---
title: "Day Ten"
date: 2026-02-19 16:30:00 +0000
categories: [Projects]
tags: [plan-B, foi]
---
I'm continuing to make progress and have added a processing step to extract verbatim text for each exemption argument and conclusion from the Decision Notices to allow the summaries to be "fact checked" on the page. This was really straightforward, as I can verify the content against the original source. At the moment, I’ve set this to be collapsed by default to keep the UI clean, but it's easily accessible where needed.

As the tasks for the sprint are done, I’ve had time to watch the background noise of the internet. It's been a while since I've done anything like this, and I am testing some fairly harsh WAF rules to see what happens. Even with no real footprint or content, the volume of "bad" traffic is much higher than I remember it being. It feels like a sign of the times. A lot of it is clearly AI driven, though not labeled as such, and I guess there will be more of this as autonomous agents become more common. 

With that in mind, I am wondering if I should actually be making it easier for genuine well behaved agents to find things. I find the idea of [serving them markdown](https://blog.cloudflare.com/markdown-for-agents/) to be an interesting one, and more appealing than the llms.txt idea.

I'm also weighing up design choices around mixed outcome public interest tests. At the moment, users can browse all tests within a single DN, but by default, each test that covers a specific exemption and outcome can be viewed separately. I’m considering if this might be more confusing than helpful. The navigation needs to stay intuitive though, and seeing two results with contradictory outcomes next to each other might not be ideal. I could combine these and have multiple cards per page, but that would lose something I think. The Granularity vs simplicity trade-off is as old as time. Something to think about nonetheless.
